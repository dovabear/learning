D1_V2/2153
1core
3.5G

hdnn 172.23.24.12 
hdrm 172.23.24.13
spma 172.23.24.14 

D4S_V3/6298
4cores
16G

workera 172.23.24.15 
workerb 172.23.24.16

nano /etc/hosts
hdnn 172.23.24.12
hdrm 172.23.24.13
spma 172.23.24.14
workera 172.23.24.15
workerb 172.23.24.16

==手動安裝hdfs==
cd /opt
wget http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.9.0/hadoop-2.9.0.tar.gz
tar -zxvf hadoop-2.9.0.tar.gz
檢視相容性 file /opt/hadoop-2.9.0/lib/native/libhadoop.so.1.0.0

nano /opt/myhadoop.sh
#/bin/bash

export JAVA_HOME=/usr/lib/jvm/java-8-oracle
export HADOOP_HOME=/opt/hadoop-2.9.0
export PATH=$PATH:$JAVA_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin

nano ~/.bashrc
source /opt/myhadoop.sh

source ~/.bashrc


一、設定hdfs
1.設定core-site.xml
nano /opt/hadoop-2.9.0/etc/hadoop/core-site.xml
<property>
<name>fs.defaultFS</name>
<value>hdfs://master:8020</value>
</property>

2.設定hdfs-site.xml
nano /opt/hadoop-2.9.0/etc/hadoop/hdfs-site.xml
<property>
<name>dfs.replication</name>
<value>2</value>
</property>
<property>
<name>dfs.namenode.name.dir</name>
<value>/root/master</value>
</property>
<property>
<name>dfs.permissions.supergroup</name>
<value>root</value>
</property>
--------datanode只設定這些-----------------------
<property>
<name>dfs.datanode.data.dir</name>
<value>/root/dn</value>
</property>
-------------------------------


3.設定hadoop-env.sh
nano /opt/hadoop-2.9.0/etc/hadoop/hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/java-8-oracle
export HADOOP_HEAPSIZE=128
export HADOOP_NAMENODE_INIT_HEAPSIZE="128"
export HADOOP_CLIENT_OPTS="-Xmx128m $HADOOP_CLIENT_OPTS"
export HADOOP_LOG_DIR=/tmp
--------datanode設定還要再加上這些---------------------------
export HADOOP_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS $HADOOP_DATANODE_OPTS -client"
-----------------------------------


全部重開
在nanenode
hdfs namenode -format
hadoop-daemon.sh start namenode
hadoop-daemon.sh start secondarynamenode
在datanode
hadoop-daemon.sh start datanode

在各台執行
hdfs dfsadmin -report
jps

關掉hdfs
在nanenode
hadoop-daemon.sh stop namenode
hadoop-daemon.sh stop secondarynamenode
在datanode
hadoop-daemon.sh stop datanode


=====設定yarn=========
1.設定core-site.xml，hdfs已經設定過了
nano /opt/hadoop-2.9.0/etc/hadoop/core-site.xml
<property>
<name>fs.defaultFS</name>
<value>hdfs://master:8020</value>
</property>

2.設定yarn-site.xml
nano /opt/hadoop-2.9.0/etc/hadoop/yarn-site.xml
<property>
<name>yarn.resourcemanager.hostname</name>
<value>rm</value>
</property>
------nodemanager再加上這段--------------------
<property>
<name>yarn.nodemanager.local-dirs</name>
<value>/root/yarn</value>
</property>
<property>
<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
--------------------------

3.設定yarn-env.sh
nano /opt/hadoop-2.9.0/etc/hadoop/yarn-env.sh
JAVA_HEAP_MAX=-Xmx128m
YARN_HEAPSIZE=128
export YARN_RESOURCEMANAGER_HEAPSIZE=128
-----nodemanager再加上這段--------------------------
export YARN_NODEMANAGER_OPTS="-client"
-------------------------------
在# default log directory & file這一行下加上
export YARN_LOG_DIR=/tmp

4.設定mapred-site.xml(空白檔案)
nano /opt/hadoop-2.9.0/etc/hadoop/mapred-site.xml
<?xml version="1.0"?>
<configuration>
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
</configuration>

5.設定mapred-env.sh(nodemanager不用做)
nano /opt/hadoop-2.9.0/etc/hadoop/mapred-env.sh
export HADOOP_JOB_HISTORYSERVER_HEAPSIZE=128
export HADOOP_MAPRED_LOG_DIR="/root/jhslog"


在rm上執行(stop為關閉)
yarn-daemon.sh start resourcemanager
mr-jobhistory-daemon.sh start historyserver

在nodemanager上執行(stop為關閉)
yarn-daemon.sh start nodemanager

yarn node all -list